# AI Agent Guidelines — {project_name}

**Goal:** Fast, reliable changes with tight quality gates.

<tools_and_mcp>
## Recommended Tools & MCP Servers

### Essential Tools for AI Agents
- **Code Analysis**: Use project-specific analysis tools for understanding codebase structure
- **File Operations**: Prefer `Read`, `Write`, `Edit` tools over shell commands for file manipulation
- **Search**: Use `Grep` and `Glob` tools for efficient code searching
- **Testing**: Use `Bash` tool for running tests and quality checks
- **Version Control**: Use `Bash` tool for git operations (status, diff, commit)

### Recommended MCP Servers
- **Sequential Thinking**: For complex reasoning and planning tasks
- **Filesystem**: For advanced file operations and directory traversal
- **Git**: For sophisticated version control operations
- **Database**: If your project uses databases for development

### Tool Usage Guidelines
- **Read before Edit**: Always read files before making changes
- **Batch Operations**: Use multiple tool calls in parallel when possible
- **Quality Checks**: Run linting and formatting after code changes
- **Test Integration**: Verify changes with test execution
- **Documentation**: Update relevant documentation when making significant changes

### MCP Configuration
See `.mcp.json` for current server configuration. Add new servers as needed for project-specific requirements.
</tools_and_mcp>

<quick_checklist>
## Quick Checklist for a Contribution
✅ **Core Logic**: Pure function? No global config? Explicit args?
✅ **Error Handling**: Used specific exceptions correctly?
✅ **Tests**: Added a new test for the change?
✅ **Quality Gates**: Ran `ruff check --fix`, `ruff format`, `pytest`?
✅ **Documentation**: Updated relevant docs and session log?
</quick_checklist>

<core_principles>
## I. Core Principles

### Golden Rules

1.  **Single Responsibility**: One module for one job (data processing, business logic, presentation).
    - *Why*: Easier to test, debug and reuse. Changes in one part don't break others.

2.  **Separate Concerns**: I/O and error handling at boundaries; core logic must be pure and testable.
    - *Why*: Pure logic can be tested without DB/files. Easier to find bugs in separate layers.

3.  **Small Interfaces**: Pass explicit arguments; no hidden globals or entire config objects.
    - *Why*: All function dependencies are visible. Easier to understand what's needed for testing.

4.  **Stable Contracts**: Use Pydantic models for data with typed, documented fields.
    - *Why*: Automatic validation. IDE shows errors before runtime. Fewer runtime bugs.

5.  **Fail Fast**: Raise narrow, specific exceptions. Never use `except Exception:` or return silent defaults.
    - *Why*: Errors don't propagate through system. Faster to find source of problem.

### Required Workflow
**analyze → plan → code → lint → format → test → if OK → log session & commit | else → fix**
</core_principles>

<quality_gates>
## II. Quality Gates

### Commands
```bash
ruff check --fix src/ tests/
ruff format src/ tests/
pytest -q
```

### Definition of Done
*   Lint clean and correctly formatted.
*   All tests passing (including new, minimal tests for changes).
*   A `SESSION.md` entry is appended.
*   Commit message follows Conventional Commits format (`feat:`, `fix:`, etc.).

### Example Workflow
```bash
# 1. Analyze the problem
# Read existing code, understand the codebase structure

# 2. Plan the solution
# Break down into: core functions → formatting → orchestration

# 3. Code with layer separation
# core/ - pure logic
# formatting/ - presentation
# orchestration/ - coordination

# 4. Quality gates
ruff check --fix src/ tests/  # Fix style issues automatically
ruff format src/ tests/       # Format code consistently
pytest -q                    # Run tests (must pass)

# 5. Document and commit
echo "feat: add new feature" >> SESSION.md
git add . && git commit -m "feat: add new feature"
```
</quality_gates>

<architecture_patterns>
## III. Architecture Patterns

### Layer Separation Pattern

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Data Layer    │    │  Core Logic     │    │ Presentation    │
│                 │    │                 │    │                 │
│ • I/O Operations│────▶│ • Pure Logic    │────▶│ • Formatting    │
│ • Validation    │    │ • Computations  │    │ • Error Display │
│ • Error Convert │    │ • Fail Fast     │    │ • HTML/Reports  │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

### Boundary vs Logic (keep logic pure)

```python
from pathlib import Path

class ProcessingError(Exception): ...

def load_text(path: Path) -> str:
    """Boundary function: Handles I/O and converts low-level errors."""
    try:
        return path.read_text(encoding="utf-8")
    except (FileNotFoundError, UnicodeDecodeError) as e:
        raise ProcessingError(f"Error loading {{path}}: {{e}}") from e
```

```python
import ast

def analyze_content(text: str) -> dict:
    """Pure logic function: Assumes valid input, raises specific errors."""
    tree = ast.parse(text)  # May raise SyntaxError, which is expected to be handled upstream.
    funcs = [n.name for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]
    classes = [n.name for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]
    return {{"functions": funcs, "classes": classes}}
```

### Small Interface (pass only what's needed)

```python
def format_tree(files: list, show_comments: bool = False) -> str:
    """This function only needs a list of files and a boolean, not the whole config."""
    lines = []
    for f in files:
        line = f"├─ {{f.rel_path}}"
        if show_comments and getattr(f, 'comment', None):
            line += f"  # {{f.comment}}"
        lines.append(line)

    if lines:
        lines[-1] = lines[-1].replace("├", "└", 1)
    return "\n".join(lines)
```
</architecture_patterns>

<error_handling>
## IV. Error Handling Strategy

### Exception Types by Layer
- **Core Layer**: `InsufficientDataError`, `DataValidationError`, `ColumnMappingError`
- **Processing Layer**: Re-raise core errors, add `ProcessingError` for workflow issues
- **Presentation Layer**: Catch all specific errors, log appropriately, show placeholders

### Error Handling Examples

**❌ BAD - Silent failures:**
```python
def compute_metrics(data):
    if data.empty:
        return None  # Silent failure
    # ... logic
```

**✅ GOOD - Fail fast:**
```python
def compute_metrics(data):
    if data.empty:
        raise InsufficientDataError("No data for metrics computation")
    # ... logic
```

**✅ GOOD - Centralized error handling:**
```python
def add_section(self, title, builder_func):
    try:
        section_data = builder_func()
        self._add_section_html(title, section_data)
    except InsufficientDataError as e:
        logger.info(f"Skipping section '{{title}}': {{e}}")  # Expected situation
        self._add_placeholder(title, "Insufficient data")
    except (DataValidationError, ProcessingError) as e:
        logger.warning(f"Data issue in section '{{title}}': {{e}}")  # Config problem
        self._add_placeholder(title, "Data error")
```

### Testing Exception Handling
```python
def test_compute_metrics_raises_insufficient_data():
    """Test that empty data raises appropriate exception."""
    empty_data = []

    with pytest.raises(InsufficientDataError, match="No data for metrics computation"):
        compute_metrics(empty_data)

def test_compute_metrics_raises_validation_error():
    """Test that invalid data raises appropriate exception."""
    invalid_data = ["not", "valid", "format"]

    with pytest.raises(DataValidationError, match="Invalid data format"):
        compute_metrics(invalid_data)
```
</error_handling>

<configuration_management>
## V. Configuration Management

### Extract-Transform-Pass Pattern

**❌ BAD: Hidden dependencies**
```python
def build_feature_section(cfg, ctx):
    # Function signature doesn't show what config fields are actually needed
    result = compute_feature_metrics(ctx.data, cfg)  # Hidden coupling
```

**✅ GOOD: Explicit dependencies**
```python
def build_feature_section(cfg, ctx):
    # Extract phase: Make dependencies explicit
    mapping_cfg = cfg.column_mapping
    feature_cfg = cfg.feature_settings

    # Transform phase: Call core with specific parameters
    result = compute_feature_metrics(
        data=ctx.data,
        id_column=mapping_cfg.id_field,
        value_column=mapping_cfg.value_field,
        min_samples=feature_cfg.min_samples
    )

    # Pass phase: Format and return
    formatted = format_feature_table(result)
    return create_table_section("Feature Analysis", formatted)
```

### Standard Section Builder Pattern
```python
def build_section_name(cfg, ctx):
    """
    Standard template for all build_* functions.

    Args:
        cfg: Application configuration object
        ctx: Runtime context with data

    Returns:
        Section object with formatted content

    Raises:
        InsufficientDataError: When input data is empty or invalid
        DataValidationError: When configuration is invalid
    """
    # Step 1: Extract parameters with type annotations
    mapping_cfg = cfg.column_mapping
    specific_cfg = cfg.specific_feature

    # Step 2: Check and analyze
    if ctx.data is None or len(ctx.data) == 0:
        raise InsufficientDataError("No data for section")

    result = core_analysis_function(ctx.data, mapping_cfg.field, specific_cfg.param)

    # Step 3: Format and return
    formatted = format_function(result)
    return create_section("Title", formatted)
```
</configuration_management>

<anti_patterns>
## VI. Anti-patterns & Common Mistakes

### ❌ Mixed Concerns
```python
def build_data_table(events, id_col, name_col):
    counts = compute_counts(events, id_col, name_col)
    counts = counts.rename(columns={{"action": "Action"}})  # Formatting in analysis!
    return counts
```

### ✅ Separated Concerns
```python
# core/
def compute_action_distribution(events, id_col, name_col):
    return counts  # Standard English columns

# formatting/
def format_action_table(counts_df):
    return counts_df.rename(columns={{"action": "Action"}})
```

### ❌ Config Overuse
```python
# Don't pass entire config to core functions
def compute_metrics(events, config):  # Too much coupling
    id_col = config.column_mapping.id_field  # Hidden dependency
```

### ✅ Explicit Parameters
```python
def compute_metrics(events, id_col: str, value_col: str):  # Clear interface
    pass
```

### ❌ "God Functions"
```python
def build_comprehensive_analysis(cfg, ctx):
    # 500+ lines of mixed concerns
    # Data loading + analysis + formatting + error handling
    # Impossible to test or reuse
```

### ✅ Focused Functions
```python
def build_analysis_section(cfg, ctx):
    # Extract configuration
    data = extract_analysis_data(ctx, cfg.data_config)

    # Analyze with pure function
    results = analyze_data(data, cfg.analysis_params)

    # Format for display
    formatted = format_results(results, cfg.display_config)

    return create_section("Analysis", formatted)
```
</anti_patterns>

<project_context>
## VII. Project Context

### Architecture Flow
Add your project's main workflow here.

### Module Mapping
*   **Core Logic**: `src/{{project_name}}/core/`
*   **Processing**: `src/{{project_name}}/processing/`
*   **Output Generation**: `src/{{project_name}}/formatting/`
*   **Data Models**: `src/{{project_name}}/models.py`

### Data Flow Pattern
```
Configuration → Core Analysis → Formatting → Output Generation → Error Handling
      ↓              ↓             ↓              ↓               ↓
[Validation]   [Pure Logic]   [Styling]    [HTML/JSON]    [Graceful UX]
```
</project_context>

<contribution_guide>
## VIII. Contribution & Logging

*   Tests are mandatory → see **[TESTS.md](TESTS.md)**
*   System design principles → see **[ARCHITECTURE.md](ARCHITECTURE.md)**
*   Track live progress in **[TODO.md](TODO.md)** (gitignored)
*   After completion, append notes to **[SESSION.md](SESSION.md)**
*   **Debugging Hint**: Check **[SESSION.md](SESSION.md)** for similar issues fixed previously

### Design Decision Framework

**Question 1**: Can this logic be tested without external dependencies?
- **Yes**: Put in core layer
- **No**: Put in data/I/O layer

**Question 2**: Does this involve user-facing display formatting?
- **Yes**: Put in formatting layer
- **No**: Keep in core with standard output format

**Question 3**: Does this coordinate multiple core functions?
- **Yes**: Put in orchestration layer
- **No**: Keep as focused core function

**Question 4**: Does this handle multiple error types from lower layers?
- **Yes**: Put in presentation layer (error boundary)
- **No**: Let specific errors bubble up to appropriate handler
</contribution_guide>